{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitanaconda3virtualenv4d1d8b461d6a4699bf7db9d788d48898",
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape , concatenate\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling1D, Conv1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.generic_utils import Progbar\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "\n",
    "np.random.seed(1337)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data_x = data[data.Class == 0]\n",
    "data_x.shape\n",
    "data_x = data_x.drop(['Time','Class'], axis = 1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data['normalizedAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data = data.drop(['Amount','Time'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training for Classification\n",
    "X = data.iloc[:, data.columns != 'Class']\n",
    "y = data.iloc[:, data.columns == 'Class']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VISUALISATION\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data by each feature\n",
    "\n",
    "axarr = [[]]*len(data.columns)\n",
    "columns = 4\n",
    "rows = int( np.ceil( len(data.columns) / columns ) )\n",
    "f, fig = plt.subplots( figsize=(columns*4, rows*3) )\n",
    "\n",
    "f.suptitle('Distribution Plots', size=16)\n",
    "\n",
    "for i, col in enumerate(data.columns[:]):\n",
    "    axarr[i] = plt.subplot2grid( (int(rows), int(columns)), (int(i//columns), int(i%columns)) )\n",
    "    axarr[i].hist( [ data.loc[ data.Class == 0, col ], data.loc[ data.Class == 1, col ] ], label=['non-fraud','fraud'], \n",
    "                          bins=np.linspace( np.percentile(data[col],0.1), np.percentile(data[col],99.9), 30 ),\n",
    "                          normed=True )\n",
    "    axarr[i].set_xlabel(col, size=12)\n",
    "    axarr[i].set_ylim([0,0.8])\n",
    "    axarr[i].tick_params(axis='both', labelsize=10)\n",
    "    if i == 0: \n",
    "        legend = axarr[i].legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    if i%4 != 0 : \n",
    "        axarr[i].tick_params(axis='y', left='off', labelleft='off')\n",
    "    else:\n",
    "        axarr[i].set_ylabel('Values',size=12)\n",
    "\n",
    "plt.tight_layout(rect=[0,0,1,0.95]) # xmin, ymin, xmax, ymax\n",
    "# plt.savefig('plots/Engineered_Data_Distributions.png')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "from sklearn import decomposition\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "np.random.seed(77)\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.Amount = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "X_x = df.iloc[:, df.columns != 'Class']\n",
    "y_y = df.iloc[:, df.columns == 'Class']\n",
    "X_names = df.columns.values[:-1]\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "forest.fit(X_x, y_y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_x.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, X_names[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize= (15, 5 ))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_x.shape[1]), importances[indices],\n",
    "       color=\"blue\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_x.shape[1]), X_names[indices])\n",
    "plt.xlim([-1, X_x.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_names[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 4 features\n",
    "\n",
    "top_4 = X_names[indices][:6]\n",
    "print(top_4)\n",
    "\n",
    "X_top_4 = df[top_4]\n",
    "print(X_top_4)\n",
    "\n",
    "top4df = pd.concat([df['Time'], X_top_4, y_y], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top4df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    col1= top4df.columns.values[i+1]\n",
    "    print(col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "THIS IS WHERE THE PREDICTION MODEL ENDS\n",
    "\n",
    "NEXT: NEURAL NETWORK MODEL\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = top4df.groupby('Class')\n",
    "print(data['V12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units =20 , kernel_initializer = 'uniform', activation = 'relu', input_dim = 29))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "score = classifier.evaluate(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the generator model\n",
    "def get_generative(G_in, dense_dim=20, out_dim=29, lr=1e-3):\n",
    "    x = Dense(dense_dim)(G_in)\n",
    "    x = Activation('tanh')(x)\n",
    "    G_out = Dense(out_dim, activation='tanh')(x)\n",
    "    G = Model(G_in, G_out)\n",
    "    opt = SGD(lr=lr)\n",
    "    G.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return G, G_out\n",
    "\n",
    "G_in = Input(shape=(10,))\n",
    "G, G_out = get_generative(G_in)\n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Building the Discriminator model\n",
    " def get_discriminative(D_in, lr=1e-3, drate=.25, n_channels=20, conv_sz=3, leak=.2):\n",
    "    x = Reshape((-1, 1))(D_in)\n",
    "    x = Conv1D(n_channels, conv_sz, activation='relu')(x)\n",
    "    x = Dropout(drate)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(n_channels)(D_in)\n",
    "    D_out = Dense(2, activation='sigmoid')(x)\n",
    "    D = Model(D_in, D_out)\n",
    "    dopt = Adam(lr=lr)\n",
    "    D.compile(loss='binary_crossentropy', optimizer=dopt)\n",
    "    return D, D_out\n",
    "\n",
    "D_in = Input(shape=[29])\n",
    "D, D_out = get_discriminative(D_in)\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainability(model, trainable=False):\n",
    "    model.trainable = trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = trainable\n",
    "        \n",
    "def make_gan(GAN_in, G, D):\n",
    "    set_trainability(D, False)\n",
    "    x = G(GAN_in)\n",
    "    GAN_out = D(x)\n",
    "    GAN = Model(GAN_in, GAN_out)\n",
    "    GAN.compile(loss='binary_crossentropy', optimizer=G.optimizer)\n",
    "    return GAN, GAN_out\n",
    "\n",
    "GAN_in = Input([10])\n",
    "GAN, GAN_out = make_gan(GAN_in, G, D)\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Building GAN model\n",
    "def sample_data_and_gen(G, n_samples=10000, noise_dim=10):\n",
    "    random_indices = np.random.choice(199363, size=n_samples, replace=False)\n",
    "    xx_train = np.array(X_train)\n",
    "    XT = xx_train[random_indices]\n",
    "    XN_noise = np.random.uniform(0, 1, size=[n_samples, noise_dim])\n",
    "    #print(XN_noise)\n",
    "    XN = G.predict(XN_noise)\n",
    "    X = np.concatenate((XT, XN))\n",
    "    y = np.zeros((2*n_samples, 2))\n",
    "    y[:n_samples, 1] = 1\n",
    "    y[n_samples:, 0] = 1\n",
    "    return X, y\n",
    "# Pretrain the Discriminator and Generator\n",
    "def pretrain(G, D, noise_dim=10, n_samples=10000, batch_size=32):\n",
    "    X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)\n",
    "    #print(X,y)\n",
    "    set_trainability(D, True)\n",
    "    D.fit(X, y, epochs=200, batch_size=32)\n",
    "\n",
    "pretrain(G, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to sample from latent space\n",
    "def sample_noise(G, noise_dim=10, n_samples=1000):\n",
    "    X = np.random.uniform(0, 1, size=[n_samples, noise_dim])\n",
    "    y = np.zeros((n_samples, 2))\n",
    "    y[:, 1] = 1\n",
    "    return X, y\n",
    "\n",
    "# Training the GAN model \n",
    "def train(GAN, G, D, epochs=400, n_samples=1000, noise_dim=10, batch_size=32, verbose=False, v_freq=50):\n",
    "    d_loss = []\n",
    "    g_loss = []\n",
    "    e_range = range(epochs)\n",
    "    if verbose:\n",
    "        e_range = tqdm(e_range)\n",
    "    for epoch in e_range:\n",
    "        X, y = sample_data_and_gen(G, n_samples=n_samples, noise_dim=noise_dim)\n",
    "        print(X.shape)\n",
    "        set_trainability(D, True)\n",
    "        d_loss.append(D.train_on_batch(X, y))\n",
    "        \n",
    "        X, y = sample_noise(G, n_samples=n_samples, noise_dim=noise_dim)\n",
    "        \n",
    "        set_trainability(D, False)\n",
    "        g_loss.append(GAN.train_on_batch(X, y))\n",
    "        if verbose and (epoch + 1) % v_freq == 0:\n",
    "            print(\"Epoch #{}: Generative Loss: {}, Discriminative Loss: {}\".format(epoch + 1, g_loss[-1], d_loss[-1]))\n",
    "    return d_loss, g_loss\n",
    "\n",
    "d_loss, g_loss = train(GAN, G, D, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting training  error of the generator and discriminator\n",
    "\n",
    "ax = pd.DataFrame({'Generative Loss': g_loss, 'Discriminative Loss': d_loss, }).plot(title='Training loss', logy=True)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VIEWED_SAMPLES = 2\n",
    "X_gen , y_gen = sample_data_and_gen(G, n_samples=100000)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gen = X_gen[100000:, :]\n",
    "print(X_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label for the generated/fake data\n",
    "yg = np.zeros((len(X_gen), 1))\n",
    "yg[:] = 0\n",
    "#print(yg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted label\n",
    "y_pred = classifier.predict(X_gen)\n",
    "print(\"Number of data samples wrongly classified: \", len(y_pred[y_pred[:] >= 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples considered real by the inital classifier\n",
    "print(len(y_pred[y_pred[:] >= 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of the classifier on the training data\n",
    "y_pred = (y_pred > 0.5)\n",
    "score = classifier.evaluate(X_gen, yg)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confution matrix of \n",
    "confusion_matrix(y_pred, yg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = np.hstack((X_gen, yg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract column names\n",
    "col_names = X.columns.values\n",
    "col_names = np.append(col_names, [\"Class\"])\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of the generated data\n",
    "df_gen = pd.DataFrame(data=gen_data, columns=col_names)\n",
    "df_gen.Class = 0\n",
    "df_gen.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch positions of column tags\n",
    "def df_column_switch(df, column1, column2):\n",
    "    i = list(df.columns)\n",
    "    a, b = i.index(column1), i.index(column2)\n",
    "    i[b], i[a] = i[a], i[b]\n",
    "    df = df[i]\n",
    "    return df\n",
    "\n",
    "data = df_column_switch(data, 'Class', 'normalizedAmount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud  = data[data.Class == 1]\n",
    "df_gen.Class = 2\n",
    "gen_data = pd.concat([data,df_gen ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot of Generated data\n",
    "\n",
    "axarr = [[]]*len(df_gen.columns)\n",
    "columns = 4\n",
    "rows = int( np.ceil( len(df_gen.columns) / columns ) )\n",
    "f, fig = plt.subplots( figsize=(columns*4, rows*3) )\n",
    "\n",
    "f.suptitle('Distribution Plots', size=16)\n",
    "\n",
    "for i, col in enumerate(data.columns[:]):\n",
    "    axarr[i] = plt.subplot2grid( (int(rows), int(columns)), (int(i//columns), int(i%columns)) )\n",
    "    axarr[i].hist( [ gen_data.loc[ gen_data.Class == 0, col ], gen_data.loc[ gen_data.Class == 1, col ], gen_data.loc[ gen_data.Class == 2, col ] ], label=['non-fraud','fraud', 'generated'], \n",
    "                          bins=np.linspace( np.percentile(data[col],0.1), np.percentile(data[col],99.9), 40 ),\n",
    "                          normed=True )\n",
    "    axarr[i].set_xlabel(col, size=12)\n",
    "    axarr[i].set_ylim([0,0.8])\n",
    "    axarr[i].tick_params(axis='both', labelsize=10)\n",
    "    if i == 0: \n",
    "        legend = axarr[i].legend()\n",
    "        legend.get_frame().set_facecolor('white')\n",
    "    if i%4 != 0 : \n",
    "        axarr[i].tick_params(axis='y', left='off', labelleft='off')\n",
    "    else:\n",
    "        axarr[i].set_ylabel('Values',size=12)\n",
    "\n",
    "plt.tight_layout(rect=[0,0,1,0.95]) # xmin, ymin, xmax, ymax\n",
    "# plt.savefig('plots/Engineered_Data_Distributions.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud  = data[data.Class == 1]\n",
    "nonfraud  = data[data.Class == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen.Class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for the second classifier\n",
    "train_gen = pd.concat([nonfraud.sample(10000), fraud.sample(150), df_gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split for the second classifier model\n",
    "\n",
    "X_train_new = train_gen.iloc[:, train_gen.columns != 'Class']\n",
    "y_train_new = train_gen.iloc[:, train_gen.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_new[y_train_new.Class == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier1 = Sequential()\n",
    "classifier1.add(Dense(units =20 , kernel_initializer = 'uniform', activation = 'relu', input_dim = 29))\n",
    "classifier1.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier1.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "classifier1.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "classifier1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# training a classifier on fake data\n",
    "classifier1.fit(X_train_new, y_train_new, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting and Evaluating of the validation set using the GAN model\n",
    "y_pred = classifier1.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "score = classifier1.evaluate(X_test, y_test)\n",
    "score\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for GAN model\n",
    "\n",
    "np.random.seed(77)\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df.Amount = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "\n",
    "X_names = train_gen.columns.values[:-1]\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "forest.fit(X_train_new, y_train_new)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_x.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, X_names[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize= (15, 5 ))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_x.shape[1]), importances[indices],\n",
    "       color=\"blue\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_x.shape[1]), X_names[indices])\n",
    "plt.xlim([-1, X_x.shape[1]])\n",
    "plt.show()"
   ]
  }
 ]
}